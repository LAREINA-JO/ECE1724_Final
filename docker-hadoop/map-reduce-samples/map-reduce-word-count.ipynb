{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple WordCount Example for MapReduce using Hadoop Streaming and Python\n",
    "\n",
    "By: Vahid Mostofi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move input files to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/inputs/data/shakespeare.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -mkdir -p /outputs\n",
    "!hadoop fs -mkdir -p /inputs\n",
    "!hadoop fs -put data /inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### list the content of the HDFS folder we just created\n",
    "The three files we created are now stored on the HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/outputs/result': No such file or directory\n",
      "WARNING: Use of this script to execute dfs is deprecated.\n",
      "WARNING: Attempting to execute replacement \"hdfs dfs\" instead.\n",
      "\n",
      "Found 2 items\n",
      "drwxr-xr-x   - root supergroup          0 2023-11-21 02:42 /inputs/data/.ipynb_checkpoints\n",
      "-rw-r--r--   3 root supergroup    5450373 2023-11-21 02:42 /inputs/data/shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /outputs/result\n",
    "!hadoop dfs -ls /inputs/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapper\n",
    "this is the word count example, so we need to create the mapper and reducer.\n",
    "\n",
    " * the ```%%writefile mapper.py``` tells Jupyter to save the content of the cell as a file named mapper.py in the same direcotry. So the #!/opt/bit.... is the first line of the file.\n",
    " * the ```#!/opt/bitnami/python/bin/python``` specifies the Python path for running the file.\n",
    " * the mapper reads from the stdin, which is provided to it by the MapReduce framework and also writes to stdout (using print)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "import sys\n",
    "import string\n",
    "\n",
    "for line in sys.stdin: # reads from stdin\n",
    "    print(\"your message B\", file=sys.stderr)\n",
    "    line = line.strip()\n",
    "    for i in string.punctuation:\n",
    "        line = line.replace(i,' ')\n",
    "    line = line.lower()\n",
    "    words = line.split()\n",
    "\n",
    "    for word in words: # writes to stdout\n",
    "        if word[0] < \"a\" :\n",
    "            reducer = 0\n",
    "        elif \"a\" <= word[0] and word[0] < \"e\":\n",
    "            reducer = 1\n",
    "        elif \"e\" <= word[0] and word[0] < \"l\":\n",
    "            reducer = 2\n",
    "        elif \"l\" <= word[0] and word[0] < \"r\":\n",
    "            reducer = 3\n",
    "        else:\n",
    "            reducer = 4\n",
    "        output = str(reducer)+\"-\"+word\n",
    "        print(\"%s\\t%d\" % (output, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reducer\n",
    "similarly to previous cell, here we create a file, named ```reducer.py``` and store the logic for our reducer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/opt/bitnami/python/bin/python\n",
    "# -*-coding:utf-8 -*\n",
    "\n",
    "import sys\n",
    "total = 0\n",
    "lastword = None\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    reducer, new_line = line.split(\"-\")\n",
    "    # recuperer la cle et la valeur et conversion de la valeur en int\n",
    "    word, count = new_line.split()\n",
    "    count = int(count)\n",
    "\n",
    "    # passage au mot suivant (plusieurs cles possibles pour une même exécution de programme)\n",
    "    if lastword is None:\n",
    "        lastword = word\n",
    "    if word == lastword:\n",
    "        total += count\n",
    "    else:\n",
    "        print(\"%s\\t%d occurences\" % (lastword, total))\n",
    "        total = count\n",
    "        lastword = word\n",
    "\n",
    "if lastword is not None:\n",
    "    print(\"%s\\t%d occurences\" % (lastword, total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run MapReduce\n",
    "now to we need to run the map reduce program using our mapper.py and reducer.py\n",
    "\n",
    " * ```!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar ``` specifies the path for streaming module of MapReduce\n",
    " * ```-file``` tells map-reduce which files should be moved to the worker nodes, you can also move txt files and read them in your mapper.py or reducer.py\n",
    " * ```-mapper``` and ```-reducer``` specify the the commands for mapper and reducer, because mapper.py file has the path to python as the first line, the system would know how to execute it.\n",
    " * ```-input``` tells which folder should be scanned for input, all the files in this folder would be fed to the mappers (mapper.py) as stdin\n",
    " * ```-output``` specifies the path for the folder which the output of the map-reduce execution should be stored. Remmeber the folder must be empty, in other words every single execution of the following command needs a new folder. You don't need to crate the folder before hand, just make sure you use a new path each time.\n",
    " \n",
    " * for more infomration about map-reduce streaming API please use  http://hadoop.apache.org/docs/r1.2.1/streaming.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-21 03:23:48,378 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/training/mapper.py, /training/reducer.py, /tmp/hadoop-unjar1623727409404518037/] [] /tmp/streamjob8606559555682019331.jar tmpDir=null\n",
      "2023-11-21 03:23:48,921 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "2023-11-21 03:23:49,025 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2023-11-21 03:23:49,042 INFO client.RMProxy: Connecting to ResourceManager at resourcemanager/172.18.0.3:8032\n",
      "2023-11-21 03:23:49,042 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.18.0.5:10200\n",
      "2023-11-21 03:23:49,147 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1700533158708_0005\n",
      "2023-11-21 03:23:49,218 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:49,277 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:49,697 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:50,164 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-11-21 03:23:50,182 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:50,200 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:50,206 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "2023-11-21 03:23:50,227 INFO Configuration.deprecation: mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "2023-11-21 03:23:50,228 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2023-11-21 03:23:50,228 INFO Configuration.deprecation: map.output.key.field.separator is deprecated. Instead, use mapreduce.map.output.key.field.separator\n",
      "2023-11-21 03:23:50,275 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "2023-11-21 03:23:50,283 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1700533158708_0005\n",
      "2023-11-21 03:23:50,283 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2023-11-21 03:23:50,382 INFO conf.Configuration: resource-types.xml not found\n",
      "2023-11-21 03:23:50,383 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2023-11-21 03:23:50,624 INFO impl.YarnClientImpl: Submitted application application_1700533158708_0005\n",
      "2023-11-21 03:23:50,643 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1700533158708_0005/\n",
      "2023-11-21 03:23:50,644 INFO mapreduce.Job: Running job: job_1700533158708_0005\n",
      "2023-11-21 03:23:54,690 INFO mapreduce.Job: Job job_1700533158708_0005 running in uber mode : false\n",
      "2023-11-21 03:23:54,690 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2023-11-21 03:24:00,734 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2023-11-21 03:24:01,739 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2023-11-21 03:24:03,748 INFO mapreduce.Job:  map 100% reduce 20%\n",
      "2023-11-21 03:24:07,763 INFO mapreduce.Job:  map 100% reduce 40%\n",
      "2023-11-21 03:24:10,775 INFO mapreduce.Job:  map 100% reduce 60%\n",
      "2023-11-21 03:24:12,782 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "2023-11-21 03:24:14,789 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2023-11-21 03:24:15,799 INFO mapreduce.Job: Job job_1700533158708_0005 completed successfully\n",
      "2023-11-21 03:24:15,850 INFO mapreduce.Job: Counters: 54\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=162421\n",
      "\t\tFILE: Number of bytes written=2020773\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5454669\n",
      "\t\tHDFS: Number of bytes written=505259\n",
      "\t\tHDFS: Number of read operations=31\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=10\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=5\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=20836\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=59568\n",
      "\t\tTotal time spent by all map tasks (ms)=5209\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7446\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=5209\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=7446\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=21336064\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=60997632\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=122395\n",
      "\t\tMap output records=911193\n",
      "\t\tMap output bytes=8253174\n",
      "\t\tMap output materialized bytes=225545\n",
      "\t\tInput split bytes=200\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=23686\n",
      "\t\tReduce shuffle bytes=225545\n",
      "\t\tReduce input records=911193\n",
      "\t\tReduce output records=23686\n",
      "\t\tSpilled Records=1822386\n",
      "\t\tShuffled Maps =10\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=10\n",
      "\t\tGC time elapsed (ms)=263\n",
      "\t\tCPU time spent (ms)=9040\n",
      "\t\tPhysical memory (bytes) snapshot=2061025280\n",
      "\t\tVirtual memory (bytes) snapshot=52200910848\n",
      "\t\tTotal committed heap usage (bytes)=1719664640\n",
      "\t\tPeak Map Physical memory (bytes)=475619328\n",
      "\t\tPeak Map Virtual memory (bytes)=5071478784\n",
      "\t\tPeak Reduce Physical memory (bytes)=253030400\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8416509952\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5454469\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=505259\n",
      "2023-11-21 03:24:15,851 INFO streaming.StreamJob: Output directory: /outputs/result\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar /opt/hadoop-3.2.1/share/hadoop/tools/lib/hadoop-streaming-3.2.1.jar \\\n",
    "    -D map.output.key.field.separator=- \\\n",
    "    -D mapred.text.key.partitioner.options=-k1,1 \\\n",
    "    -D mapred.reduce.tasks=5 \\\n",
    "    -file $PWD/mapper.py\\\n",
    "    -file $PWD/reducer.py\\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input /inputs/data \\\n",
    "    -output /outputs/result \\\n",
    "    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lets look at the output of the map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2023-11-21 03:24 /outputs/result/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup     105518 2023-11-21 03:24 /outputs/result/part-00000\r\n",
      "-rw-r--r--   3 root supergroup     104584 2023-11-21 03:24 /outputs/result/part-00001\r\n",
      "-rw-r--r--   3 root supergroup     151472 2023-11-21 03:24 /outputs/result/part-00002\r\n",
      "-rw-r--r--   3 root supergroup       2877 2023-11-21 03:24 /outputs/result/part-00003\r\n",
      "-rw-r--r--   3 root supergroup     140808 2023-11-21 03:24 /outputs/result/part-00004\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /outputs/result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-21 03:24:18,568 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "e\t119 occurences\n",
      "each\t238 occurences\n",
      "eager\t9 occurences\n",
      "eagerly\t3 occurences\n",
      "eagerness\t1 occurences\n",
      "eagle\t27 occurences\n",
      "eagles\t9 occurences\n",
      "eaning\t1 occurences\n",
      "eanlings\t1 occurences\n",
      "ear\t199 occurences\n",
      "earing\t1 occurences\n",
      "earl\t155 occurences\n",
      "earldom\t5 occurences\n",
      "earlier\t1 occurences\n",
      "earliest\t3 occurences\n",
      "earliness\t1 occurences\n",
      "earls\t11 occurences\n",
      "early\t50 occurences\n",
      "earn\t13 occurences\n",
      "earned\t2 occurences\n",
      "earnest\t38 occurences\n",
      "earnestly\t9 occurences\n",
      "earnestness\t4 occurences\n",
      "earns\t1 occurences\n",
      "ears\t157 occurences\n",
      "earth\t317 occurences\n",
      "earthen\t1 occurences\n",
      "earthlier\t1 occurences\n",
      "earthly\t32 occurences\n",
      "earthquake\t6 occurences\n",
      "earthquakes\t1 occurences\n",
      "earthy\t6 occurences\n",
      "eas\t4 occurences\n",
      "ease\t54 occurences\n",
      "eased\t2 occurences\n",
      "easeful\t1 occurences\n",
      "eases\t1 occurences\n",
      "easier\t9 occurences\n",
      "easiest\t1 occurences\n",
      "easiliest\t1 occurences\n",
      "easily\t28 occurences\n",
      "easiness\t3 occurences\n",
      "easing\t2 occurences\n",
      "east\t41 occurences\n",
      "eastcheap\t15 occurences\n",
      "easter\t1 occurences\n",
      "eastern\t6 occurences\n",
      "eastward\t1 occurences\n",
      "easy\t65 occurences\n",
      "eat\t12023-11-21 03:24:19,999 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "l\t23 occurences\n",
      "la\t78 occurences\n",
      "laban\t2 occurences\n",
      "label\t2 occurences\n",
      "labell\t1 occurences\n",
      "labienus\t1 occurences\n",
      "labio\t1 occurences\n",
      "labor\t10 occurences\n",
      "laboring\t2 occurences\n",
      "labors\t1 occurences\n",
      "labour\t95 occurences\n",
      "laboured\t4 occurences\n",
      "labourer\t2 occurences\n",
      "labourers\t1 occurences\n",
      "labouring\t15 occurences\n",
      "labours\t10 occurences\n",
      "laboursome\t2 occurences\n",
      "labras\t1 occurences\n",
      "labyrinth\t2 occurences\n",
      "lac\t5 occurences\n",
      "lace\t7 occurences\n",
      "laced\t1 occurences\n",
      "lacedaemon\t2 occurences\n",
      "laces\t1 occurences\n",
      "lacies\t1 occurences\n",
      "lack\t119 occurences\n",
      "lackbeard\t1 occurences\n",
      "lacked\t3 occurences\n",
      "lackey\t8 occurences\n",
      "lackeying\t1 occurences\n",
      "lackeys\t4 occurences\n",
      "lacking\t7 occurences\n",
      "lacks\t16 occurences\n",
      "lad\t28 occurences\n",
      "ladder\t19 occurences\n",
      "ladders\t3 occurences\n",
      "lade\t1 occurences\n",
      "laden\t3 occurences\n",
      "ladies\t145 occurences\n",
      "lading\t2 occurences\n",
      "lads\t23 occurences\n",
      "lady\t927 occurences\n",
      "ladybird\t1 occurences\n",
      "ladyship\t44 occurences\n",
      "ladyships\t1 occurences\n",
      "laer\t62 occurences\n",
      "laertes\t47 occurences\n",
      "lafeu\t118 occurences\n",
      "lag\t6 occurences\n",
      "lagging\t1 occurence2023-11-21 03:24:21,470 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "r\t92 occurences\n",
      "rabbit\t4 occurences\n",
      "rabble\t13 occurences\n",
      "rabblement\t2 occurences\n",
      "race\t17 occurences\n",
      "rack\t21 occurences\n",
      "rackers\t1 occurences\n",
      "racket\t1 occurences\n",
      "rackets\t1 occurences\n",
      "racking\t1 occurences\n",
      "racks\t2 occurences\n",
      "radiance\t3 occurences\n",
      "radiant\t8 occurences\n",
      "radish\t2 occurences\n",
      "rafe\t1 occurences\n",
      "raft\t1 occurences\n",
      "rag\t7 occurences\n",
      "rage\t121 occurences\n",
      "rages\t12 occurences\n",
      "rageth\t1 occurences\n",
      "ragg\t1 occurences\n",
      "ragged\t24 occurences\n",
      "raggedness\t1 occurences\n",
      "raging\t19 occurences\n",
      "ragozine\t3 occurences\n",
      "rags\t16 occurences\n",
      "rah\t1 occurences\n",
      "rail\t37 occurences\n",
      "railed\t1 occurences\n",
      "railer\t1 occurences\n",
      "railest\t2 occurences\n",
      "raileth\t1 occurences\n",
      "railing\t8 occurences\n",
      "rails\t6 occurences\n",
      "raiment\t8 occurences\n",
      "rain\t67 occurences\n",
      "rainbow\t3 occurences\n",
      "raineth\t5 occurences\n",
      "raining\t1 occurences\n",
      "rainold\t1 occurences\n",
      "rains\t1 occurences\n",
      "rainy\t4 occurences\n",
      "rais\t24 occurences\n",
      "raise\t47 occurences\n",
      "raised\t8 occurences\n",
      "raises\t2 occurences\n",
      "raising\t6 occurences\n",
      "raisins\t1 occurences\n",
      "rak\t1 occurences\n",
      "rake\t4 occurences\n",
      "rakers\t1 occurences\n",
      "rakes\t2023-11-21 03:24:22,915 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "1\t90 occurences\n",
      "10\t5 occurences\n",
      "100\t1 occurences\n",
      "101\t1 occurences\n",
      "102\t1 occurences\n",
      "103\t1 occurences\n",
      "104\t1 occurences\n",
      "105\t1 occurences\n",
      "106\t1 occurences\n",
      "107\t1 occurences\n",
      "108\t1 occurences\n",
      "109\t1 occurences\n",
      "11\t3 occurences\n",
      "110\t1 occurences\n",
      "111\t1 occurences\n",
      "112\t1 occurences\n",
      "113\t1 occurences\n",
      "114\t1 occurences\n",
      "115\t1 occurences\n",
      "116\t1 occurences\n",
      "117\t1 occurences\n",
      "118\t1 occurences\n",
      "119\t1 occurences\n",
      "12\t3 occurences\n",
      "120\t1 occurences\n",
      "121\t1 occurences\n",
      "122\t1 occurences\n",
      "123\t1 occurences\n",
      "124\t1 occurences\n",
      "125\t1 occurences\n",
      "126\t1 occurences\n",
      "127\t1 occurences\n",
      "128\t1 occurences\n",
      "129\t1 occurences\n",
      "13\t3 occurences\n",
      "130\t1 occurences\n",
      "131\t1 occurences\n",
      "132\t1 occurences\n",
      "133\t1 occurences\n",
      "134\t1 occurences\n",
      "135\t1 occurences\n",
      "136\t1 occurences\n",
      "137\t1 occurences\n",
      "138\t1 occurences\n",
      "139\t1 occurences\n",
      "14\t2 occurences\n",
      "140\t1 occurences\n",
      "141\t1 occurences\n",
      "142\t1 occurences\n",
      "143\t1 occurences\n",
      "144\t1 occurences\n",
      "145\t1 occurences\n",
      "146\t1 occurences\n",
      "147\t1 occurences\n",
      "148\t1 occurences\n",
      "149\t1 occurences\n",
      "15\t2 occurences\n",
      "150\t1 occurences\n",
      "151\t1 occurences\n",
      "152\t1 occurences\n",
      "153\t1 occur2023-11-21 03:24:24,348 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "a\t14725 occurences\n",
      "aaron\t97 occurences\n",
      "abaissiez\t1 occurences\n",
      "abandon\t10 occurences\n",
      "abandoned\t2 occurences\n",
      "abase\t2 occurences\n",
      "abash\t1 occurences\n",
      "abate\t14 occurences\n",
      "abated\t3 occurences\n",
      "abatement\t3 occurences\n",
      "abatements\t1 occurences\n",
      "abates\t1 occurences\n",
      "abbess\t25 occurences\n",
      "abbey\t20 occurences\n",
      "abbeys\t1 occurences\n",
      "abbominable\t1 occurences\n",
      "abbot\t9 occurences\n",
      "abbots\t1 occurences\n",
      "abbreviated\t1 occurences\n",
      "abed\t8 occurences\n",
      "abel\t2 occurences\n",
      "aberga\t2 occurences\n",
      "abergavenny\t8 occurences\n",
      "abet\t1 occurences\n",
      "abetting\t1 occurences\n",
      "abhominable\t1 occurences\n",
      "abhor\t19 occurences\n",
      "abhorr\t12 occurences\n",
      "abhorred\t10 occurences\n",
      "abhorring\t2 occurences\n",
      "abhors\t4 occurences\n",
      "abhorson\t21 occurences\n",
      "abide\t37 occurences\n",
      "abides\t8 occurences\n",
      "abilities\t5 occurences\n",
      "ability\t10 occurences\n",
      "abject\t11 occurences\n",
      "abjectly\t1 occurences\n",
      "abjects\t2 occurences\n",
      "abjur\t2 occurences\n",
      "abjure\t4 occurences\n",
      "able\t59 occurences\n",
      "abler\t1 occurences\n",
      "aboard\t28 occurences\n",
      "abode\t9 occurences\n",
      "aboded\t1 occurences\n",
      "abodements\t1 occurences\n",
      "aboding\t1 occurences\n",
      "abominable\t14 o"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -head /outputs/result/part-00000\n",
    "!hdfs dfs -head /outputs/result/part-00001\n",
    "!hdfs dfs -head /outputs/result/part-00002\n",
    "!hdfs dfs -head /outputs/result/part-00003\n",
    "!hdfs dfs -head /outputs/result/part-00004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a directory out of HDFS and store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-21 03:24:26,225 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\r\n"
     ]
    }
   ],
   "source": [
    "!rm -r /outputs/res_out_of_hdfs\n",
    "!mkdir -p /outputs/res_out_of_hdfs\n",
    "!hdfs dfs -copyToLocal /outputs/result/* /outputs/res_out_of_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS  part-00000  part-00001  part-00002  part-00003  part-00004\r\n"
     ]
    }
   ],
   "source": [
    "!ls /outputs/res_out_of_hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e\t119 occurences\n",
      "each\t238 occurences\n",
      "eager\t9 occurences\n",
      "eagerly\t3 occurences\n",
      "eagerness\t1 occurences\n",
      "eagle\t27 occurences\n",
      "eagles\t9 occurences\n",
      "eaning\t1 occurences\n",
      "eanlings\t1 occurences\n",
      "ear\t199 occurences\n",
      "l\t23 occurences\n",
      "la\t78 occurences\n",
      "laban\t2 occurences\n",
      "label\t2 occurences\n",
      "labell\t1 occurences\n",
      "labienus\t1 occurences\n",
      "labio\t1 occurences\n",
      "labor\t10 occurences\n",
      "laboring\t2 occurences\n",
      "labors\t1 occurences\n",
      "r\t92 occurences\n",
      "rabbit\t4 occurences\n",
      "rabble\t13 occurences\n",
      "rabblement\t2 occurences\n",
      "race\t17 occurences\n",
      "rack\t21 occurences\n",
      "rackers\t1 occurences\n",
      "racket\t1 occurences\n",
      "rackets\t1 occurences\n",
      "racking\t1 occurences\n",
      "1\t90 occurences\n",
      "10\t5 occurences\n",
      "100\t1 occurences\n",
      "101\t1 occurences\n",
      "102\t1 occurences\n",
      "103\t1 occurences\n",
      "104\t1 occurences\n",
      "105\t1 occurences\n",
      "106\t1 occurences\n",
      "107\t1 occurences\n",
      "a\t14725 occurences\n",
      "aaron\t97 occurences\n",
      "abaissiez\t1 occurences\n",
      "abandon\t10 occurences\n",
      "abandoned\t2 occurences\n",
      "abase\t2 occurences\n",
      "abash\t1 occurences\n",
      "abate\t14 occurences\n",
      "abated\t3 occurences\n",
      "abatement\t3 occurences\n"
     ]
    }
   ],
   "source": [
    "!head /outputs/res_out_of_hdfs/part-00000\n",
    "!head /outputs/res_out_of_hdfs/part-00001\n",
    "!head /outputs/res_out_of_hdfs/part-00002\n",
    "!head /outputs/res_out_of_hdfs/part-00003\n",
    "!head /outputs/res_out_of_hdfs/part-00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /outputs/result\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /outputs/result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
